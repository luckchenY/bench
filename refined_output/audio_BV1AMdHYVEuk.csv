question,response
这个正则项其实就是VAE的正则项，就是它其实就是希望表征能够尽可能平滑一些， 那么如果不加这个正则项的话，它可能就变成了一个 AE。 然后它的隐空间的表征可能就 不是非常的平滑，对于后面的  无论对于后面的任务学习策略学习都不是很好。,对。
在第一项的话，就是相当于看  起来好像是一个叫做基于条件的这种VAE是吧？,对。
"那 里面, 在第一项里面， 就是比如说VAE里面的话， 你采样  两个 observation嘛，好像你  又没有说把它重建成.  然后把部分编码成隐向量.",呃  这是这个公式的第一项，就是。 哦， 对。 我们重建的就是整张图吗？  对。  但是就是 就是比如说这个VAE里面，你采  样出一个，前面采样了三个编码， 三个这个 这三个观测码， 这个VAE里面，它不是应该就是你采  样这个 把它全部重建成？
就是你这个工作是说我要重建这张图在其他视角下的那个样子。  对。 然后呢，就是  我就是说从下面这个公式来看的话，就是说。 对。  这个  这个好像就是跟VAE好像不太一样，就是感觉看起来。,这一项其实就是VAE的重建损失嘛。就是最大化我们的后验嘛，然后这一项其实就是   因为我们是基于VQVAE训的，其实。 所以 这项其实是VQVAE的正则化损失，然后这一项的话，就是我们的对比，对比损失。
在第一项里面的话，你看  在第一项里面，就是比如说在VAE里面的话，它是说我把这个SIVN，把这个东西把它 迁入到一个隐变量， 然后从隐变量把它减码出来。  对，是是他的重建， 但你这个看见  又相当于是说，好像是把其他两个东西做成隐变量  以后呢，然后去把另外一个东西把它减码出来。,对对，是这样。 但是，但是其实我们是， 是有对本身的  本身的重建了， 就是比如， 比如说我们这张图， 我们是会 基于这张图的，这张图的 viewing variant encoding和这张图的 view dependent encoding来重，重建这张图了，。  就是说我们，我们是有自身的重建，只不过说，我们 在自身的重建之外，我们会额外引入一些  混杂起来的，混杂起来的那个  表征，然后去重建对应的图。
就是自身重建的那个， 相当于没有写在这个地方。   啊，  自身重建其实是包含的这个里面，因为你看就是这边是SIVJ   呃 就是这边SIVJ 和 SMV，它这里没有规定说这个   这个SIVJ和SMV，它不能一样，就是其实可以一样的。,因为我们是从这个 从这个  batch中采的嘛。 对，
从  从采的， 我理解，就是说， 相当于S是两个视角嘛，  然后SM那个样本的话是拆来一个N视角嘛，这三个是不一样的。,他它不一定不一样，就是SM 可以跟S一样。  对  实际操作的时候，其实我们是这样，就是，我们是sample一个类似于矩阵一样的batch， 这个矩阵在这个维度上，它的视角是一样的，在这个维度上，它的状态是一样的。  然后我们首先会针对， 针对每个举证中的每个元素，各自提取它的VAE和VDE， 然后来做一个自身的重建， 就是你刚说的自身重建， 然后我们会对这个 举证不同的维度进行shuffle， 比如说， 我们在  比如我们在view的view的这个维度上shuffle之后，我们我们希望的是它的， 就是比如说我们shuffle完，我们希望用 思考一下，就是 比如说我们在  比如说我们在view的维度上shuffle了，那么它对应的状态，对应的状态因为是不变的，所以我们希望提取出来的VAE是一样的，VRE是一样的。
然后我们想要， 比如说  对于 那个同一个view的这，同一个view的这个层面来shuffle之后， 我们希望不同的view shuffle到这个这个地方，它跟原先的，原先的这个位置的VDE，去重建， 也能重建出这张图来。  其实大概的意思就是最后总结出来意思其实就是这样一个式子。,就是  有交叉的重建， 有部分的重建。
