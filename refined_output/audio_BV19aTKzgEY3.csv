question,response
在推理侧，我们其实在推理的时候不需要之前的历史的这个query，只需要历史的这个value和K，这个value和K就需要缓存下来，那么推理侧如果随着文本长度变得越来越长，这个value和K它的cash会变得越来越大，并且那个可能会导致我们的这个GPU一类的设备，然后存储空间不够用，然后会把它往这个下级的存储设备去转移，这样的话就会导致整个的系统时间变很长，所以说后面出了一个那个改进工作，其实就是这个最右侧的这个multi-query attention，那它的方式比较极端就是说，就是既然K和V需要cash，并且这个量比较大，那我就直接把它这个全部共享，这样的话只留一个，这样它确实会变得少很多，但带来了这个问题，就是multi-query attention，一般情况下这个性能相较于multi-head attention是下降的比较多的，后面又对这个GTA做了个升级，得到了GTA，GTA做的一个升级主要是对这个value这个地方做了个升级，这个地方还有个和MAA不一样的地方在于，我们这个地方用的是一个非线性projection。,我们会把这个query group起来，然后这样的话可以实现一个multihead和multiquery之间的一个均衡。
对于GQA到GTA的改进，就像刚开始讲的，就是因为KV cash太多了，所以说我们通过group的方式把这个KV cash的一个，就是共享，然后让它的这个cash的量变少，这是一个方法。,因为这个K和value其实都做了一个shear的一个操作，并且这个query和K之间的这个Attention的计算也变少了。
MLAA会带来计算量变大，它是实际上是通过增加计算去换取这个cash的压缩，它的那个增加的点，就是首先第一个就是它的每一个头，就是它对比的时候，每一个头其实都是正常的一个attention的1.5倍，就是一般来说我们的头都是128，但是，那个MLAA一般会为了加一个那个rope的query，它会变成一个64，所以说它就是128+64，变成了192，就是它的1.5倍，一般是这么设置的，而且它这个投影实际上也是和这个长度相关的，所以说它也额外增加了一部分这个的运算，所以如果去比prefer的速度，其实MLAA是比这个GQA和MHA可能还是要慢一些的，但是由于它降低了这个KV cash，就是在解码的时候，它的速度会变快，然后这是现在大家比较常用的吧，就是GQA和这个MLAA这两个框架。,我们就是可以再用一下group query attention的方式，然后再把这个query group一下，就是把这个value也做一个shear，然后这样的话就可以得到一个group head attention。
对于GQA和这个MHA，就是在降低了cash和降低了Attention计算的情况下，我们这个模型依旧保持了这个GQA差不多一个性能。,就是通过配置一系列的这个参数，让它和后面加的这一项这个2NKDL就是保持差不多的级别，然后这样的话它的线性计算的那个量级也是差不多的。
理论计算量，主要分析是attention计算里面，它有两个核心的组成部分，第一个是线性投影，第二个是那个QKV的那个attention计算，这个的话，可能看数值有点不太好懂，就是我稍微讲一下，就是以MHA和GQA为例，attention这个都是N方的一个计算，然后前面主要取决于系数，然后MHA和GQA它的系数都是2NHdh，我们这个是NKDK+DL，DK一般是我们在设置的时候，这个DK一般是和DH是相等的，就像刚才我们GVA里面设置的时候，就是DK和DH是相等的，然后DL是这个DK的两倍，所以说我们这个地方算出来应该是一加二，这个地方应该是3倍的DH，但是我们这个地方的NQ和这个地方的NH去做这个比较的时候，我们这个一般选取了NQ是这个NH的四分之一。,这样的话就是算完之后，我们应该是一个四分之三，而它前面应该是2NHdh，这样的话一比的话应该是一个，就是我们只有它的一个八分之三算一个比值。
这一类关于RM架构的优化的工作是非常宝贵的，因为这里面涉及到的一些东西特别硬核，我就想问一下，就是在现阶段大家，基于一个特定的语言模型架构去训练，然后这个时侯我们又提出一个全新的，非常底层的一个架构创新，那么这个时候这一类的架构创新怎么能够应用在他们现在已有的后训练或者是预训练的浪潮之中，比如说我们需要重新就是有非常大量的这个文本去重新训练，还是说我们有一些其他的，比如说一些这种转移的方法，能让它不不需要再重新经过预训练后训练这样庞大的计算量，孙博士在这方面有怎样的思考？,如果有一个新框架出来之后，我们有一套就是策略能让它尽快的把这个新框架应用起来，可能会更好，MLAA的话，我了解到是有一个工作的，他那个工作是把那个就是常规的GQA去转成一个MLAA，但是那个操作，它其实是一个数学上的一个等价变化，因为他们这个地方都是一个线性投影，所以说是比较好转的，我们这个多加了一个非线性，所以可能是我们有方法去直接转移过去的，但是应该也需要一定量的微调，但这需要一些实验去验证，就是怎么去更高效率的把这个框架做一个迁移。
