question,response
在做这个强化学习分享的时候，或者多智能体分享的时候，有些人听完之后可能会问一个问题，就是这个A模型是大模型实现的吗？,这个问题其实我觉得就是和多智能体强学学习可能关系不大，但是这个问题也提出了一个新的想法，就是这个大模型到底能不能打这个星际争霸？控制这个多智能体打这个争霸。
在之前的训练过程中，对于这个多智能体小队来说，我们会发现强化学学习，一般来说得到这个策略的先验性比较差，然后呢可能需要数百完步或者是上千万步的这个训练部署，同时呢，这个深度学习，学习出来的这个模型，它的可解性也不高。,那如果反过来说，如果我们用这个大模型生成一个决策树的话，那它和这个强化学学习相比，那决策树它首先是一个白盒决策，然后呢，这个大模型生成决策树的这个过程，可能是一轮或者是多轮的这个inference。
决策树它自身是具备可解释性的，那么它和强化学学习可能是完全不同的另一个方向。,那如果采用这种这种大模型生成决策树的这个方式的话，那么看大模型确实是可以去打的。
如果让大模型去打赢一个场景，那么这个大模型可能首先需要去规划一个策略或者规划多个策略，然后呢，大模型同时需要有就是生成这个python代码，生成这个决策脚步的这个能力。,然后，这个大模型应该还具备一个结果分析与判别的能力。
如果把这三个能力放在一个大模型里面去解决的话，那对于大模型的挑战来说，就是比较大的。,"所以呢，在我们这个工作中，我们是把它分成了三个模块，分别是这个plannner, coeder和creative。"
那首先说这个planner，那我们需要把这一个场景下有多少个角色，有多少个敌人，然后每一个角色它当前的一个状态，比如它有多少血量，有多少护盾，它的这个运动的速度有多大。,把这些东西都要交给大模型，然后大模型通过去分析这些数据去得到一个这个所谓的这个策略，这个strategy的skeleton。
那我们做的也是大概相似的，就是我们先把每一个角色的信息，然后把这个地图信息和这个运动信息。,那再加上这个大模型，这个历史以来都采用过哪些的策略，把它全部都交给这个planner，然后让planner去得到一个一个这个策略和它的一个概。
比如说，在当前的这个场景之下，他可能得到的一个这个 defend run，就是得到一个走砍，然后在什么情况下需要用到这个策略，以及怎样去实现这个策略。,嗯。
那planner得出了，给出了这个strategy之后，那么由一个coder负责具体的实现。,"那么最近来说的话，比如说这个coder吧，它之前的2.5,现在出了3，那么，大模型生成代码的这个能力现在已经有了很大的提高，那我们就用大模型去生成一些符合pythonSCR2的，这个符合这个包的这个代码。"
那下面呢，这两个是两个生成代码的例子。,那这个因为是决策树嘛，它是具备一定的那个可解释性的，所以我们能够看出来。
比如我们先获取了这个stoker，然后获取了敌方的SP，然后，如果这个stoker在SP的这个范围之内，那么stoker就去attack去打。,那如果的话呢，这个stoker的move，朝向那个SP的位置去运动，然后同时呢，如果这个stoker他的那个武器的这个冷却大于零，就是他刚刚攻击攻击之后，我们给他设定一个撤退的这个位置，然后他移动到撤退的位置。
那么当前的这样的一个策略，其实就已经可以打败这个已经可以完成，就是SMC里面的2S，VS1C，能够完成这样的一个场景。,但是，这个场景下，又又采用这个策略，他有的时候本可以超。
所以呢，这个 creative 会对当前的这个代码实现增加一些一些新的这个策略或者是提升一下这个策略。,那么右边的这部分其实和左边的，大概就比较相似，那它多了一些这个红色的部分，那么比如说，除了前面的这个武器的冷却之外，它还增加了一些包括这个这个盾的。
这个恢复和这个生命值的这个这样的一个判定，然后把这个repeat distance由三变成了五，所以它可以更向后去走一走，然后呢，也增加了一个good priding的。,。
那么采用了右边的这个策略之后，我们可以认为在这个2SVSC这个场景下，其实可以百分百能够保赢。,。
因为coder生成的这个这个脚本，它有可能是bug的。,如果它没有bug的话，它真正的去在这个画面里面进行，它可能也，它的胜率可能也不会很高，有可能不会很高。
所以呢，我们用这个creative这个模块去让他去去检测一下，让他去提升一下，提供一些改进。,。
那依旧是，比如说左右的这个这个对比。,那么左边的这个实现，这个code的实现是这个 hiding，这个放风筝的这个这个策略。
那这个策略其实核心就是向这个地图中心的返方向运动六个位置。,。
那么如果一直采用这个方式去拍ding的话，那么它会被地图边缘卡住。,对。
就是虽然这个风筝的这个策略是没问题的，但是限于它具体的实现，它也未必能够有好的结果。,。
那么经过creative的修改之后呢，嗯，这个大模型给出了一个解决办法，就是引入了一个方向，然后引入了一个角度，然后让这个智能体或者让这个agent，他每一次在在那个撤退的时候，都以一个九十度的这个角度去跑，那这样的话呢，它相当于就成现了一个圆形的这个行为，这样的话它就不会被地图卡住。,。
那么这个呢，也是3SV3Z，VS4Z和VS5Z的解决办法。,。
那么这个我们用了两张动图啊，然后也大概展现了一下，就是制定的这个图信息之后，这个大模型根据agent的具体的具体的角色去得到不一样的这个策略吧。,那比如，左边的这个是2CVs64CG，那两个剧象，因为他具备这个，这个攀爬，这个悬崖的能力，所以这两个剧象就可以在悬崖上下，来回走，然后去打这个，这个屌丝。
那么右边第二个这个动图呢，其实是这个corridor的这个图。,那么给到这个图片信息之后，那agent就可以控制这些zlock，堵在这个，摆的位置然后去打。
这三个场景，其实用的是同一套策略。,那这个策略就是其中的一个sock负责，这个 run。
那么另外两个sock负责这个激火这个最弱的，或者是激火这个血量最低的。,。
我们发现，这个策略其实是一个相对来说通用的策略，那么它在这个三个场景下都有比较好的结果。,。
那也说明它这个策略是具备一定的建议性的。,OK。
那前面是这个SMC，大模型打SMC，那么这个第二个工作，其实就是我们推出一个这个SMC hard的这样一个场景。,。
那之前我们说大模型，既然可以打SMC，那么下面是不是就可以用强化学习去打大模型。,那么由于原来的SMC，它限定的比较死，就是不可以编辑这个对手策略。
那么另外一个原因呢，就是SMCv1，它的这个触点和范围不够，那现在绝大多数的ML算法在打SMC的时候，可能都能够在1000万步，达到将近100%的胜率。,那么也就是说，SMC这个场景对于多正体算法的这个判别的这个能力其实已经不太够了。
那么包括这个VR也是一样的，那虽然VR引入了触手化设定，但是呢这两个环境都是用默认的单一定的脚本。,。
那如果打开这个SCR map，打开这个文件，我们会发现，这个对手策略其实是在这个地图里面，那比如说第一个策略就是控制所有的角色，控制player2的所有的角色，去这个attack，去攻击这个1的位置。,。
那包括这个SMCVR其实也是控制每一个角色去攻击，去攻击CLOES unit，离它最近的这个角色。,。
那这样固定的策略会导致一些问题吧。,就比如说，先看右边这个问题，那右边这个是始终攻击最近的角色的话，就可以用一个走带的这个智能体。
那么左边的这个就是唯一的问题，我们会发现，三个这个这个在被卡在1这个位置，因为他出了这个范围。,所以说，如果面对这个面对单一的或者是有问题的策略的话，那智能体。
那智能体学到的策略模型就会更容易够腻，得到某一个策略上。,那或者呢，会利用到这个对手策略的一些bug，然后你得到一些bug策略。
然后，对于对手。,嗯。
那如果生成了多个脚本，我们可以用多个这个策略进行这个和。,为了更加丰富对手策略，我们可以把这个为对手 这个封装一些，比如说，观测状态，行为，奖励等接口，那么支持这个对手发展强化学学习的模型。
那么这个是我们的SMC hard这个环境的整体的这个这个结构。,那这里面其实是，首先是中心的这个这个SC，这个星际争霸这个游戏，那这个SCI提供了一些SCR的这个protocol，然后呢，这个PYSCR，是封装了这个protocol，换句话说呢，玩家可以，或者是这个这个研究者啊，可以跟通过这个PYSCR去操控，操控这个星际争霸这个环境，那比如说，启动游戏啊，这个加载地图之类的。
那这个v1版本的这个SMC，其实是PYSCR的进一步的封装。,那PYSCR提供了一些这个观测，这个low observation，然后SMC，解析了这个这个observation，生成了得到的一些，比如说OBS，这个向量化的这个action reward。
那我们，其实也可以用类似的方式，也同样的去警惕一下这个的。,为对手也得到了，为对手也封装了相似的这个接口，那么这部分就是self play的这个这个内容。
那么除了这个self play之外，我们可以说支持对手的策略，那我们说这个观测。,这个信息里面是包含了每个智能体的位置，包括他的这个血量，那么这些信息自然也是可以作为决策数的这个输入，然后让决策数生成对应的这个行为。
那一般来说SMC，它的输出的行为可能是向量化的，比如说1234。,然后这个决策数输出的行为呢，可能是，比如A点attack B，那我们在这个action，增加一层，把它翻译成这个SCR protocol能够识别的这个action。
那么我们做了一些实验，就是把当前的比较，比较困难的这个MIX，比如flex，这个Mapo和Hoop，这些，这些算法应用在我们的这个SMC harder上，去侧一下实际表现。,我们发现，在原来1000万步，将近百分百胜率的这些场景之下，那么现在呢，其实又有了比较大的提升。
那原来在v1里面是SMC的一些任务，那么在现在的这个SMC hard的这个场景之下依旧难度很大。,对，那换句话说，为多智能体强化学学习的这个平台。
那么最后呢，总结一下呢，就是SMC hard，如果说多正体强学习这个领域，已经被基训，分布执行和这个SMC这个场景被限定死了的话，那么我们希望通过推出SMC hard，为多正体强学习的这个这个社区吧，在比如这个DG1的领域，比如对手建模，比如说多智能体之博弈，包括这个大模型辅助，强化学学习，也包括策略迁移性，就是在一些这些方面，能够有一个新的策略场景。,就希望在这边会有更多的发展。
对，那么以上就是我这一次的这个汇报，然后感谢，谢谢大家。,。
呃，我这边有个人提了一个问题，就是我这个smart康，他， 偏曲能比smart提高多少的难度？,嗯  这个难度的话，如果用，还是用曲线来说吧，就是  嗯 如果，如果按曲线来说的话，那对于比如说，这个flex的这个算法来说，呃，它的这个性能的降低大概在，就是大概50%到70%这样。 那么在，比如说，原来的这些superhard的这些超，比如说68位，比如说这个30，534方面，它的性能的下降可能会更大一些。  嗯，那反过来可能就难度提高的更大一些。
那他更难学，主要就是因为它，就是对这个动作操作精细度要求是更高了吗？,还是因为他一些其他方面的改变。
那如果我们  就是，举个例子吧，就是第一个对手策略是永远攻击最近的角色，那第二个策略是永远攻击，比如对手血量最低的角色，那如果这两个策略结合在一起的话，那对于，比如，对方血量比较低的角色，那么应该怎么做吧，那如果对于那个SM1的话，它可能是退到这个队务的后面，其实就可以了。,但是面对这个混合策略的话，它可能需要退的更多一些。
对，然后，对对，这这这一方面是这个混合对手策略。,那么另一方面就是   就是，就是怎么说呢，就是  对手策略变得多了之后，这个对手策略中的这些bug，或者这种边缘的这些，这些特殊场景就会变得更少。
