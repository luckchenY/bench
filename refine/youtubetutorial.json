[
  {
    "question": "[I don't know. The beauty of um like LLMs or like the data driven models is like minimal inductive minimal human defined inductive bias and let it learn from data um but the the the causal model could also be learned, it doesn't have to be handcrafted. I mean, that's exactly what I'm talking about is not the case where the humans provide the model but where the model is learned because it's the best model or the best distribution of models that fits the data.]",
    "response": "[Yeah that, yeah. I agree with that. Like if we can find a balance and that can scale well, um I think that will be something very interesting.]"
  },
  {
    "question": "[Although, I wonder, a follow-up question on that would be, where do we get the variables or should the variables be super smart?]",
    "response": "[That's that's a great question. That's also something you don't want to hand code by hands and um the good thing with LLMs is it's teaching us that there's a very natural way that we can talk about variables uh in natural language.]"
  },
  {
    "question": "[Yeah, yeah, yeah. I wonder if they should be in a size of like a millions of variables or also this like how sparse autoencoder have yeah, combination of things.]",
    "response": "[exponential number of variables]"
  },
  {
    "question": "[Sounds great. Uh, cool, so before our exciting AGI question, we will still have one more uh one more paper on since I know a lot of us have uh constraints in what we can talk about let's talk about future tense. Um in 2025 or 2026, what papers would you look forward to? That can be in the area of further capability, it can be in the area of LM safety and then it can come from academia, it can come from industry. Who will make you excited?]",
    "response": "[Personally, I'm especially curious about interpretability. I think on the capability side, my sense is there's already a pretty good recipe that we can keep scaling out for a while. Um, I think one of the main reasons I got into neural networks is I was genuinely curious to understand how minds work and it feels like now we have really cool artificial minds and it would be interesting to know how they work. Um, and yeah, we still haven't figured out too much about that.]"
  },
  {
    "question": "[I want to go back to the interpretability suggestion I made earlier and like, it'd be great if um we could make the neural nets themselves generate their own explanations but okay, we can already do that except it's not trustworthy. So, we need to make sure, for example, that their reasoning chains and the causal explanations that the neural net produces are actually correct that they satisfy, you know, the laws of probability and logic. Um, and so if we could do that, we could end up with models that are going to be safer because they can be trustworthy. So, this this question of being self-consistent and consistent with the data. So, we already have models that are consistent with the data, but not self-consistent and and um this would be really good for safety, but also for like user interfaces and explainability in general as as a very good feature.]",
    "response": "[Yeah, there's definitely one thing that seems to be happening is that these models already tend to be quite self-consistent on their own and I think one reason is that it's just helpful for the model to be faithful. Um, most of the conclusions the models get to are representative of the chains of thought. I think maybe Yosha's going concerns are maybe about what if there's some intent that the user models have et cetera and I think those are interesting questions.]"
  },
  {
    "question": "[The reason why people are interested in understanding what is going on inside the networks is that they don't necessarily trust the answers that they're getting. Um, there could be uh, scheming issues that are, you know, beginning to attract attention and well, what do we do when we get to AGI, right, where the scheming might be more difficult to trace.]",
    "response": "[I was trying to piggy-back off what you were saying. I I really agree with this.]"
  },
  {
    "question": "[Oh, go ahead, Tom.]",
    "response": "[No, just a short remark, but this is like, this is exactly what I have in mind, where we have AI technology that actually makes us better rather than, we just delegate to AI agents. Yeah.]"
  },
  {
    "question": "[Awesome. I guess some people might have different views.]",
    "response": "[Um]"
  }
]